Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]
Traceback (most recent call last):
  File "/hpc/home/jjm132/jjm132/nlp4neuro/code/experiment_5/experiment_5.py", line 431, in <module>
    run_self_supervised_experiment()
  File "/hpc/home/jjm132/jjm132/nlp4neuro/code/experiment_5/experiment_5.py", line 378, in run_self_supervised_experiment
    model_bert = BERTCPC(input_dim, hidden_size=256, latent_dim=latent_dim).to(device)
  File "/hpc/home/jjm132/jjm132/nlp4neuro/code/experiment_5/experiment_5.py", line 239, in __init__
    self.bert = BertModel(config)
  File "/hpc/group/naumannlab/jjm132/miniconda3/envs/jjmenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 977, in __init__
    self.encoder = BertEncoder(config)
  File "/hpc/group/naumannlab/jjm132/miniconda3/envs/jjmenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 648, in __init__
    self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])
  File "/hpc/group/naumannlab/jjm132/miniconda3/envs/jjmenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 648, in <listcomp>
    self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])
  File "/hpc/group/naumannlab/jjm132/miniconda3/envs/jjmenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 563, in __init__
    self.attention = BertAttention(config)
  File "/hpc/group/naumannlab/jjm132/miniconda3/envs/jjmenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 481, in __init__
    self.self = BERT_SELF_ATTENTION_CLASSES[config._attn_implementation](
  File "/hpc/group/naumannlab/jjm132/miniconda3/envs/jjmenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 359, in __init__
    super().__init__(config, position_embedding_type=position_embedding_type)
  File "/hpc/group/naumannlab/jjm132/miniconda3/envs/jjmenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 227, in __init__
    raise ValueError(
ValueError: The hidden size (256) is not a multiple of the number of attention heads (6)
