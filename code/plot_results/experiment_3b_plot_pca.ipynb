{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f835d2e-70c7-44a3-b94c-4ff265ddcee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os, math, csv, numpy as np, torch, torch.nn as nn\n",
    "import matplotlib.pyplot as plt, matplotlib.gridspec as gridspec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"../../../external_data/data\")\n",
    "file_dir = f\"{DATA_DIR}/\"\n",
    "\n",
    "# ── CONFIGURATION ─────────────────────────────────────────────────────────\n",
    "fish_num    = 9\n",
    "hidden_size = 4096\n",
    "n_drop      = 50\n",
    "np.random.seed(0)\n",
    "\n",
    "DATA_DIR = \"/hpc/group/naumannlab/jjm132/data_prepped_for_models\"\n",
    "SAL_DIR  = f\"/hpc/group/naumannlab/jjm132/nlp4neuro/results/deepseek_only/fish{fish_num}\"\n",
    "OUT_DIR  = \"/hpc/group/naumannlab/jjm132/nlp4neuro/results/experiment3b_results/experiment_3_pca\"\n",
    "coord_dir = os.path.join(OUT_DIR, 'pc_coords')\n",
    "os.makedirs(coord_dir, exist_ok=True)\n",
    "\n",
    "# ── EMBEDDING CLASSES (as before) ───────────────────────────────────────────\n",
    "class Vanilla(nn.Module):\n",
    "    def __init__(self,D,H): super().__init__(); self.l=nn.Linear(D,H)\n",
    "    def forward(self,x): return self.l(x)\n",
    "class PosEnc(nn.Module):\n",
    "    def __init__(self,D,H,L=1024):\n",
    "        super().__init__(); self.l=nn.Linear(D,H)\n",
    "        pe=torch.zeros(L,H); pos=torch.arange(L).unsqueeze(1)\n",
    "        div=torch.exp(torch.arange(0,H,2)*(-math.log(10000.0)/H))\n",
    "        pe[:,0::2],pe[:,1::2]=torch.sin(pos*div),torch.cos(pos*div)\n",
    "        self.register_buffer('pe',pe.unsqueeze(0),persistent=False)\n",
    "    def forward(self,x): z=self.l(x); return z+self.pe[:,:z.size(1)]\n",
    "class RelPos(nn.Module):\n",
    "    def __init__(self,D,H,R=32):\n",
    "        super().__init__(); self.l=nn.Linear(D,H); self.R=R; self.rel=nn.Embedding(2*R+1,H)\n",
    "    def forward(self,x):\n",
    "        z=self.l(x); B,L,_=z.shape\n",
    "        offs=torch.arange(L,device=z.device)-torch.arange(L,device=z.device)[0]\n",
    "        offs=torch.clamp(offs,-self.R,self.R)+self.R\n",
    "        return z+self.rel(offs).unsqueeze(0).expand(B,L,-1)\n",
    "class Spectral(nn.Module):\n",
    "    def __init__(self,D,H): super().__init__(); self.l=nn.Linear(D,H); nn.init.orthogonal_(self.l.weight)\n",
    "    def forward(self,x): return torch.tanh(self.l(x))\n",
    "class SparseAE(nn.Module):\n",
    "    def __init__(self,D,H): super().__init__(); self.enc=nn.Linear(D,H)\n",
    "    def forward(self,x): B,L,_=x.shape; return torch.relu(self.enc(x.view(B*L,-1))).view(B,L,-1)\n",
    "\n",
    "def get_embeddings(D):\n",
    "    return {\n",
    "        \"Linear\":   Vanilla(D,hidden_size),\n",
    "        \"PosEnc\":   PosEnc(D,hidden_size),\n",
    "        \"RelPos\":   RelPos(D,hidden_size),\n",
    "        \"Spectral\": Spectral(D,hidden_size),\n",
    "        \"SparseAE\": SparseAE(D,hidden_size)\n",
    "    }\n",
    "order = [\"Linear\",\"PosEnc\",\"RelPos\",\"Spectral\",\"SparseAE\"]\n",
    "\n",
    "# ── LOAD DATA & SALIENCY ─────────────────────────────────────────────────────\n",
    "F   = np.load(f\"{DATA_DIR}/fish{fish_num}_neural_data_matched.npy\")\n",
    "SAL = np.load(f\"{SAL_DIR}/fish{fish_num}_importance.npy\")\n",
    "all_idx       = np.arange(F.shape[0])\n",
    "sal_sorted    = SAL.argsort()[::-1]\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# helper to tensor tokens\n",
    "def tokens(idx):\n",
    "    return torch.tensor(F[idx].T, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# ── COMPUTE OR LOAD CENTROIDS ───────────────────────────────────────────────\n",
    "centroids = {}  # embedding -> array[51×3]\n",
    "\n",
    "for emb_name in order:\n",
    "    coord_file = os.path.join(coord_dir, f\"{emb_name}_centroids.npy\")\n",
    "    if os.path.exists(coord_file):\n",
    "        centroids[emb_name] = np.load(coord_file)\n",
    "        print(f\"Loaded centroids for {emb_name} from disk.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Computing centroids for {emb_name}...\")\n",
    "    # determine baseline subset to fit PCA basis (e.g. random removal k=0)\n",
    "    rand_drop = rng.choice(all_idx, n_drop, replace=False)\n",
    "    keep0     = np.setdiff1d(all_idx, rand_drop)\n",
    "    base_tok  = tokens(keep0)\n",
    "\n",
    "    # instantiate embedding and PCA basis\n",
    "    E     = get_embeddings(keep0.size)[emb_name].eval()\n",
    "    with torch.no_grad():\n",
    "        Zb = E(base_tok).squeeze(1).cpu().numpy()\n",
    "    svd   = TruncatedSVD(n_components=3, random_state=0).fit(Zb)\n",
    "\n",
    "    # compute centroids for k=0..n_drop\n",
    "    C = np.zeros((n_drop+1, 3), dtype=float)\n",
    "    for k in range(n_drop+1):\n",
    "        drop_sal   = sal_sorted[:k]\n",
    "        candidates = np.setdiff1d(all_idx, drop_sal)\n",
    "        drop_rand  = rng.choice(candidates, n_drop-k, replace=False)\n",
    "        keep_idx   = np.setdiff1d(all_idx, np.concatenate([drop_sal, drop_rand]))\n",
    "        with torch.no_grad():\n",
    "            Zk = svd.transform(E(tokens(keep_idx)).squeeze(1).cpu().numpy())\n",
    "        C[k] = Zk.mean(0)\n",
    "    centroids[emb_name] = C\n",
    "    np.save(coord_file, C)\n",
    "    print(f\"Saved centroids for {emb_name} to {coord_file}.\")\n",
    "\n",
    "# ── PLOT CENTROIDS WITH BLUE→RED GRADIENT ───────────────────────────────────\n",
    "fig = plt.figure(figsize=(30,6), dpi=300)\n",
    "gs  = gridspec.GridSpec(1, len(order)+1, width_ratios=[1]*len(order)+[0.05], wspace=0.6)\n",
    "axes = [fig.add_subplot(gs[i], projection='3d') for i in range(len(order))]\n",
    "cax  = fig.add_axes([0.92,0.35,0.02,0.3])\n",
    "\n",
    "for idx, emb_name in enumerate(order):\n",
    "    C = centroids[emb_name]\n",
    "    ax = axes[idx]\n",
    "    for k in range(n_drop+1):\n",
    "        color = plt.cm.bwr(k/n_drop)\n",
    "        ax.scatter(*C[k], c=[color], s=60, edgecolor='k', linewidth=0.2)\n",
    "    ax.set_title(emb_name)\n",
    "    ax.set_xlabel('PC1'); ax.set_ylabel('PC2'); ax.set_zlabel('PC3')\n",
    "    pts = C\n",
    "    mr = np.ptp(pts, axis=0).max()/2.0\n",
    "    mid= pts.min(0) + np.ptp(pts,axis=0)/2.0\n",
    "    for d,m in zip('xyz', mid): getattr(ax, f'set_{d}lim')(m-mr, m+mr)\n",
    "\n",
    "import matplotlib as mpl\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=n_drop)\n",
    "cbar = mpl.colorbar.ColorbarBase(cax, cmap='bwr', norm=norm)\n",
    "cbar.set_label('# salient neurons removed')\n",
    "\n",
    "fig.subplots_adjust(left=0.04, right=0.9, top=0.95, bottom=0.05)\n",
    "out = os.path.join(OUT_DIR, 'pca_centroids_gradient.pdf')\n",
    "fig.savefig(out, transparent=True)\n",
    "print('Saved', out)\n",
    "\n",
    "# ── CSV OUTPUT ──────────────────────────────────────────────────────────────\n",
    "csv_file = os.path.join(coord_dir, 'centroid_coords.csv')\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['embedding','k_salient','pc1','pc2','pc3'])\n",
    "    for emb_name in order:\n",
    "        for k,coords in enumerate(centroids[emb_name]):\n",
    "            writer.writerow([emb_name, k, *coords])\n",
    "print('Centroid coordinates saved to', csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
